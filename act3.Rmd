---
title: "Resolución Actividad 1 máster Bioinformática UNIR (2023)"
author: "Andres Atencia Ortega; Sergio Bedoya; Marcela Casafus"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

Cargamos las librerías a utilizar

```{r}
library(tidyverse)
library(FactoMineR)
library(ggplot2)
library(nortest)
library(gtsummary)
library(factoextra)
```

## Data cleaning

```{r}
# Cargamos los datos
df <- read.csv("mubio02_act3_alimentos_nutrientes_4900.csv")
head(df)
```

```{r}
# Verificamos por datos nulos
any(is.na(df)) # Acá vemos que sí hay datos nulos

# Vemos que porcentaje representan esos nulos
#colSums(is.na(df)) / nrow(df) * 100

# Dado que solo hay nulos en 6 columnas limitamos el print a esas columnas
colSums(is.na(
  df[, c("estado_civil", "colesterol", "hdl", "HTA", "hipercolesterolemia", "hipertrigliceridemia")])
  ) / nrow(df) * 100

```

Observamos que solo hay datos nulos en 6 columnas, que los nulos representan un porcentaje muy bajo y las columnas corresponden a datos que han sido discretizados, por lo que podemos imputar estos datos por la moda.

```{r}
# Creamos una función para calcular la moda
mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# Columnas a procesar
columnas <- c("estado_civil", "colesterol", "hdl", "HTA", "hipercolesterolemia", "hipertrigliceridemia")

# Iterar sobre las columnas
for (columna in columnas) {
  # Calcular la moda de la columna actual
  moda <- mode(df[[columna]])
  
  # Reemplazar valores nulos con la moda
  df[[columna]][is.na(df[[columna]])] <- ifelse(is.na(df[[columna]]), moda, df[[columna]])
}

# Verificamos nulos
any(is.na(df))

```

Ahora procedemos a dar formato a las columnas discretizadas:
```{r}
variables <- c("sexo", "estado_civil", "tabaco", "colesterol", "hdl", "HTA", "hipercolesterolemia", "hipertrigliceridemia",
               "ECV_prev", "diab_prev", "hta_prev", "depres_prev", "FA_prev", "cancer_prev")

for (variable in variables) {
  df[[variable]] <- as.factor(df[[variable]])
}

head(df,1)
```

## PCA

Verificamos la normalidad de los datos:

```{r}
# Normalidad de los alimentos:
# Anderson-Darling para cada columna

p_values <- c() # Inicializamos un vector par guardar los p values
variables <- unlist(colnames(df[, 28:177])) # Creamos un vector con el nombre de las columnas

# Iteramos para cada alimento y nutriente aplicando el test
for (varialble in variables) {
  # Calculamos el test de normalidad y guardamos los p values en el vector creado
  p_values <- c(p_values, ad.test(df[[varialble]])$p.value)
}

# Creamos un data frame
tabla_normalidad <- data.frame(
  "Variable"=variables,
  "Test"=c("Anderson-Darling"),
  "Valor p"=p_values
  )

# Interpretamos los resultados
## Creamos una función para interpretar
interpretador <- function(pvalue) {
  if (pvalue < 0.05) {
    return("Distribucion No normal")
  } else if (pvalue > 0.05) {
    return("Distribucion Normal")
  } else {
    return("No significativo")
  }
}

## Creamos una nueva columna con la interpretación
tabla_normalidad$Interpretacion <- interpretador(tabla_normalidad$Valor.p)

# Guardamos
write.table(tabla_normalidad, file = "tabla_normalidad.csv")

# Visualizamos
tabla_normalidad
```

Realizamos el PCA

```{r}
# Análisis de componentes principales
# con scale.unit normalizamos los datos
pca.results <- PCA(X = df[,28:177], scale.unit = TRUE, graph = FALSE)
```

Calculamos la proporción de varianza explicada de cada componente (R2) y guardamos en la 2da tabla.

```{r}
# Seleccionamos la varianza explicada redondeada
p_var <- data.frame(R2 = round(pca.results$eig[, 2],2))
# Agregamos una columna para los componentes
p_var$Componentes <- row.names(p_var)

# Ordenamos las columnas como se nos solicita
p_var <- p_var %>%
  select(Componentes, R2)

# Guardamos la tabla
write.table(p_var, file = "tabla_componentes_R2.csv")

# Visualizamos
p_var
```

Graficamos la anterior tabla

```{r}
fviz_screeplot(pca.results, addlabels = TRUE, ylim = c(0, 15))
```

Vemos como la primera componente explica el 11.2% de la varianza, lo cual es más de 7 puntos porcentuales por encima del segundo componente.

Vemos el top10 variables que más contribuyen al PC1

```{r}
fviz_contrib(pca.results, choice = "var", axes = 1, top = 10, ylim = c(0,6))
```

Apreciamos que la mayoría de nutrientes contribuyen de manera similar, entre un 3% y 6% a la varianza explicada al componente principal 1. 

Vemos cuántos componentes se requieren para alcanzar un 95% de la varianza explicada

```{r}
# Seleccionamos la varianza acumulada redondeada
var_cum <- data.frame(Acumulada = round(pca.results$eig[, 3],2))

# Agregamos una columna para los componentes
var_cum$Componentes <- row.names(var_cum)

# Ordenamos las columnas como se nos solicita
var_cum <- var_cum %>%
  select(Componentes, Acumulada)

# Seleccionamos aquellos cuya varianza acumulada es mayor o igual a 95
subset(var_cum, Acumulada>=95)
```

Vemos que para explicar el 95% de la varianza necesitaríamos 113 de los 150 componentes. Y con 130 obtenemos el 99.87%.

Obtenemos las cargas de cada variable y guardamos en la tabla 3

```{r}
# Seleccionamos las cargas redondeadas
cargas <- as.data.frame(round(pca.results$var$coord,2))

# Agregamos una columna para las variables
cargas$Variable <- rownames(cargas)

# Ordenamos las columnas como se solicitan
tabla_cargas <- cargas %>%
  select(
    Variable, "Componente1"=Dim.1, "Componente2"=Dim.2,
    "Componente3"=Dim.3, "Componente4"=Dim.4, "Componente5"=Dim.5
    )

# Guardamos la tabla
write.table(tabla_cargas, file = "Tabla_cargas.csv")

# Visualizamos
tabla_cargas
```

```{r}
unique(df$sexo)
```

Graficas vistas en clases

```{r}
plot <- fviz_pca_ind(
  pca.results,
  geom = "point",
  col.ind = df$hipercolesterolemia,
  palette = c("red","green","blue","yellow","purple" ),
  addEllipses = TRUE
) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank()
  ) +
  labs(
    title = "PCA visualization by hipercolesterolemia",
    subtitle = "Principal component analysis",
    caption = "Data Source: actividad 3"
  )

print(plot)
```

```{r}
fviz_pca_biplot(
  pca.results,
  axes = c(1,2),
  geom.ind = "point",
  fill.ind = df$hipercolesterolemia,
  col.ind = "black",
  pointshape = 21,
  pointsize = 2,
  palette = NULL,
  addEllipses = TRUE,
  alpha.var = "contrib",
  col.var = "contrib",
  gradient.cols = c("red", "orange", "green"),
  repel = TRUE,
  label = "var", # cambiar a FALSE para quitar las etiquetas
  legend.title = list(
    fill = "Hipercolesterolemia",
    color = "Contribución",
    alpha = "Dirección"
  )
) +
  theme_minimal() +
  theme(
    legend.position = "right",
    legend.title = element_blank()
  ) +
  labs(
    title = "PCA visualization by hipercolesterolemia",
    subtitle = "PCA aliments and nutrients contribution",
    caption = "Data Source: actividad 3"
  )
```

```{r}
head(df,1)
```

Calculamos los terciles

```{r}
# Supongamos que 'df' tiene una columna llamada 'variable'
df$terciles <- cut(df$variable, breaks = c(-Inf, terciles, Inf), labels = c("T1", "T2", "T3"))

```
Usamos la librería gtsummary para hacer la tabla solicitada de variables sociodemograficas y descriptivas para agruparlas por componente y tercil.

```{r}
# Usamos gtsummary para hacer la tabla
tabla_descriptiva <- df_tabla4 %>%
  tbl_strata(
    strata = sexo, #agrupamos primero por sexo
    .tbl_fun = ~ .x %>%
      tbl_summary(by = categoriaIMC, #luego por la categoría de IMC
                  # Explicación abajo
                  type = list(starts_with("alimento") ~ "continuous"),
                  # Calculamos los estadísticos
                  statistic = all_continuous()~"{mean} ({sd})"
                    ) %>%
      # Modificamos el titulo
      modify_header(label = "**Variables**")%>%
      # Agregamos el test de Welch
        add_p(
          all_continuous()~"t.test",
          pvalues_fun=~style_pvalue(.x, digits = 3) # 3 digitos
          )
    ) %>%
  bold_labels()

# Visualizamos
tabla4
```


## Modelo regresión logística

## Conclusiones y recomendaciones
